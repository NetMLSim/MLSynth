# MLSynth

>https://dl.acm.org/doi/abs/10.1145/3748273.3749211

MLSynth is a tool to synthesise machine learning workloads to use in simulations.

## Install

- Requires Python 3.10+
- Install Chakra (for ET protobufs and utilities): see `https://github.com/mlcommons/chakra`.

## Usage

Generate a Chakra execution trace from a YAML config:

```bash
python .synthesise_workload.py
```

Outputs will be written under `output/<auto-name>/`.

## Config

| Parameter     | Type | Description |
|:--------------|:----:|:-----------:|
| model         | str  | Model name (e.g., `transformer`) |
| num_layers    | int  | Number of layers |
| hidden_size   | int  | Model hidden dimension |
| sequence_len  | int  | Sequence length |
| vocab_size    | int  | Vocabulary size |
| batch_size    | int  | Global batch size |
| num_microbatches | int | Microbatches per batch (pipeline) |
| bytes_per_val | int  | Bytes per tensor element (e.g., 2 for fp16) |
| scale         | float| Global scaling for compute/communication |
| parallelism.dp_size | int | Data parallel degree |
| parallelism.pp_size | int | Pipeline parallel stages |
| parallelism.tp_size | int | Tensor parallel degree |

### Example

```yaml
model:
  name: "transformer"
  num_layers: 12
  sequence_len: 2048
  vocab_size: 51200
  hidden_size: 20480
  batch_size: 32
  num_microbatches: 8
  bytes_per_val: 2
  scale: 0.01
parallelism:
  dp_size: 4
  pp_size: 4
  tp_size: 1
```

A communication group file will also be generated. This must be passed into ASTRA-sim to define which gpus belong to each collective.

## Implementing your own model

### Layer

The Layer template represents individual layers that compose neural network models. This class generates the directed graph of Chakra nodes for operations within a single layer, returning the compute operations performed during forward and backward passes. To support intra-layer parallelism, the Layer interface accepts configuration flags that modify both compute and communication operations as needed, enabling accurate modelling of distributed computation within layers.

### Model

The Model template represents complete ML models as sequences of layers. Since neural networks fundamentally consist of layer compositions, this class stores the layout of layers and propagates compute nodes generated by each layer to higher-level components.

### Orchestrator

The Orchestrator component models parallelism strategies used to distribute computation across GPUs. It contains a Model instance and orchestrates which layers are computed by each GPU, inserting communication operations between compute nodes returned by the Model class.

### Wrapper

We model performance variation via wrapper objects (i.e., Performance Wrappers) that intercept calls between the Model class and the Orchestrator class. When intercepting the calls, the performance wrapper modifies the directed graph of operations that are returned by the Model - whether this is changing FLOPs for compute nodes, or adding ’wait’ nodes to delay some operations to simulate slowdowns.